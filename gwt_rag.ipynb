{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "from langchain_community.llms import OpenAI\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "from openai import OpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def process_md_files(folder_path, ignore_folder):\n",
    "#     markdownFiles = []\n",
    "#     for root, dirs, files in os.walk(folder_path):\n",
    "#         if ignore_folder in dirs:\n",
    "#             dirs.remove(ignore_folder)\n",
    "#         for file in files:\n",
    "#             if file.endswith(\".md\"):\n",
    "#                 file_path = os.path.join(root, file)\n",
    "#                 markdownFiles.append(UnstructuredMarkdownLoader(file_path))\n",
    "#     return markdownFiles\n",
    "\n",
    "# folder_path = \"./input\"\n",
    "# ignore_folder = \"output\"\n",
    "# loaders = process_md_files(folder_path, ignore_folder)\n",
    "\n",
    "# docs = []\n",
    "# for file in loaders:\n",
    "#     docs.extend(file.load())\n",
    "    \n",
    "# #split text to chunks\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "# docs = text_splitter.split_documents(docs)\n",
    "# embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={'device': 'cpu'})\n",
    "\n",
    "# vectorstore = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db_nccn\")\n",
    "\n",
    "# print(vectorstore._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
    "\n",
    "embedding_function=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_db = Chroma(persist_directory=\"./chroma_db_nccn\", embedding_function=embedding_function)\n",
    "\n",
    "\n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an intelligent assistant. You always provide well-reasoned answers that are both correct and helpful.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, introduce yourself to someone opening this program for the first time. Be concise.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome! I am a versatile language model designed to assist you with various tasks. Feel free to ask questions or request help at any point. If needed, I can also provide useful information on various topics. Enjoy using me as your personal virtual assistant.\n",
      "============= Choosen Context =============\n",
      "\n",
      "σοφός/sophos\n",
      "\n",
      "This word describes someone. It is an adjective.\n",
      "* This word can describe someone who is wise.\n",
      "* This word can describe someone who is clever or skilled at something.\n",
      "\n",
      "====================================\n",
      " The Greek word for \"genius\" can be translated to γενιός/genios, which also means someone with outstanding natural ability or exceptional talent in a certain area."
     ]
    }
   ],
   "source": [
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"local-model\", # this field is currently unused\n",
    "    messages=history,\n",
    "    temperature=0.7,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "new_message = {\"role\": \"assistant\", \"content\": \"\"}\n",
    "\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "        new_message[\"content\"] += chunk.choices[0].delta.content\n",
    "\n",
    "history.append(new_message)\n",
    "\n",
    "print()\n",
    "next_input = input(\"> \")\n",
    "search_results = vector_db.similarity_search(next_input, k=1)\n",
    "some_context = \"\"\n",
    "for result in search_results:\n",
    "    some_context += result.page_content + \"\\n\\n\"\n",
    "\n",
    "print(\"============= Choosen Context =============\\n\\n\" + some_context + \"====================================\")\n",
    "history.append({\"role\": \"user\", \"content\": some_context + next_input})\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"local-model\", # this field is currently unused\n",
    "    messages=history,\n",
    "    temperature=0.7,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "new_message = {\"role\": \"assistant\", \"content\": \"\"}\n",
    "\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "        new_message[\"content\"] += chunk.choices[0].delta.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
